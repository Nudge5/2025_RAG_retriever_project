{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c9LQKtUPsms",
        "outputId": "d6a75e10-de11-404d-de51-8086dab67a9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: langchain 0.3.7\n",
            "Uninstalling langchain-0.3.7:\n",
            "  Successfully uninstalled langchain-0.3.7\n",
            "Found existing installation: langchain-core 0.3.18\n",
            "Uninstalling langchain-core-0.3.18:\n",
            "  Successfully uninstalled langchain-core-0.3.18\n",
            "Found existing installation: langchain-community 0.3.7\n",
            "Uninstalling langchain-community-0.3.7:\n",
            "  Successfully uninstalled langchain-community-0.3.7\n",
            "Found existing installation: langchain-openai 0.2.7\n",
            "Uninstalling langchain-openai-0.2.7:\n",
            "  Successfully uninstalled langchain-openai-0.2.7\n",
            "Found existing installation: langgraph 0.2.39\n",
            "Uninstalling langgraph-0.2.39:\n",
            "  Successfully uninstalled langgraph-0.2.39\n",
            "Found existing installation: openai 1.54.0\n",
            "Uninstalling openai-1.54.0:\n",
            "  Successfully uninstalled openai-1.54.0\n",
            "Found existing installation: faiss-cpu 1.8.0\n",
            "Uninstalling faiss-cpu-1.8.0:\n",
            "  Successfully uninstalled faiss-cpu-1.8.0\n",
            "Found existing installation: tiktoken 0.7.0\n",
            "Uninstalling tiktoken-0.7.0:\n",
            "  Successfully uninstalled tiktoken-0.7.0\n",
            "Found existing installation: pydantic 2.8.2\n",
            "Uninstalling pydantic-2.8.2:\n",
            "  Successfully uninstalled pydantic-2.8.2\n",
            "Found existing installation: packaging 24.1\n",
            "Uninstalling packaging-24.1:\n",
            "  Successfully uninstalled packaging-24.1\n",
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "! pip uninstall -y langchain langchain-core langchain-community langchain-openai langgraph \\\n",
        "  openai faiss-cpu tiktoken pydantic packaging numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQEBpt-s5VEt",
        "outputId": "82cf7f0a-2014-49d6-d66b-3053a2132709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain==0.3.27 in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-core==0.3.74\n",
            "  Downloading langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting langchain-community==0.3.27\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-openai==0.3.31\n",
            "  Downloading langchain_openai-0.3.31-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langgraph==0.2.45\n",
            "  Downloading langgraph-0.2.45-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting openai==1.100.0\n",
            "  Downloading openai-1.100.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting faiss-cpu==1.12.0\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting tiktoken==0.11.0\n",
            "  Downloading tiktoken-0.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting pypdf==5.1.0\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting python-dotenv==1.0.1\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting pydantic==2.11.7\n",
            "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy==2.0.2 in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (0.4.38)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core==0.3.74) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core==0.3.74) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core==0.3.74) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core==0.3.74) (25.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (3.13.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (0.4.3)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.0 (from langgraph==0.2.45)\n",
            "  Downloading langgraph_checkpoint-2.1.2-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.32 (from langgraph==0.2.45)\n",
            "  Downloading langgraph_sdk-0.1.74-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.100.0) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.100.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.100.0) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.100.0) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai==1.100.0) (1.3.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken==0.11.0) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.11.7) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.11.7) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.11.7) (0.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai==1.100.0) (3.11)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.27) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.27) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.100.0) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.100.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.100.0) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.3.74) (3.0.0)\n",
            "INFO: pip is looking at multiple versions of langchain-text-splitters to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain==0.3.27)\n",
            "  Downloading langchain_text_splitters-0.3.10-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.0.0->langgraph==0.2.45)\n",
            "  Downloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.2.0,>=0.1.32->langgraph==0.2.45) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.3.27) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.3.27) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.27) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.27) (1.1.0)\n",
            "Downloading langchain_core-0.3.74-py3-none-any.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m443.5/443.5 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.31-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.2.45-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m119.3/119.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.100.0-py3-none-any.whl (786 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m786.5/786.5 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\n",
            "Downloading langgraph_checkpoint-2.1.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_sdk-0.1.74-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, pypdf, ormsgpack, faiss-cpu, tiktoken, pydantic, openai, langgraph-sdk, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langgraph, langchain-community\n",
            "  Attempting uninstall: python-dotenv\n",
            "    Found existing installation: python-dotenv 1.1.1\n",
            "    Uninstalling python-dotenv-1.1.1:\n",
            "      Successfully uninstalled python-dotenv-1.1.1\n",
            "  Attempting uninstall: tiktoken\n",
            "    Found existing installation: tiktoken 0.12.0\n",
            "    Uninstalling tiktoken-0.12.0:\n",
            "      Successfully uninstalled tiktoken-0.12.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.10\n",
            "    Uninstalling pydantic-2.11.10:\n",
            "      Successfully uninstalled pydantic-2.11.10\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.109.1\n",
            "    Uninstalling openai-1.109.1:\n",
            "      Successfully uninstalled openai-1.109.1\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.79\n",
            "    Uninstalling langchain-core-0.3.79:\n",
            "      Successfully uninstalled langchain-core-0.3.79\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.11\n",
            "    Uninstalling langchain-text-splitters-0.3.11:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
            "  Attempting uninstall: langchain-openai\n",
            "    Found existing installation: langchain-openai 0.3.35\n",
            "    Uninstalling langchain-openai-0.3.35:\n",
            "      Successfully uninstalled langchain-openai-0.3.35\n",
            "  Attempting uninstall: langchain-community\n",
            "    Found existing installation: langchain-community 0.3.31\n",
            "    Uninstalling langchain-community-0.3.31:\n",
            "      Successfully uninstalled langchain-community-0.3.31\n",
            "Successfully installed faiss-cpu-1.12.0 langchain-community-0.3.27 langchain-core-0.3.74 langchain-openai-0.3.31 langchain-text-splitters-0.3.9 langgraph-0.2.45 langgraph-checkpoint-2.1.2 langgraph-sdk-0.1.74 openai-1.100.0 ormsgpack-1.11.0 pydantic-2.11.7 pypdf-5.1.0 python-dotenv-1.0.1 tiktoken-0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U \\\n",
        "  \"langchain==0.3.27\" \\\n",
        "  \"langchain-core==0.3.74\" \\\n",
        "  \"langchain-community==0.3.27\" \\\n",
        "  \"langchain-openai==0.3.31\" \\\n",
        "  \"langgraph==0.2.45\" \\\n",
        "  \"openai==1.100.0\" \\\n",
        "  \"faiss-cpu==1.12.0\" \\\n",
        "  \"tiktoken==0.11.0\" \\\n",
        "  \"pypdf==5.1.0\" \\\n",
        "  \"tqdm==4.67.1\" \\\n",
        "  \"python-dotenv==1.0.1\" \\\n",
        "  \"pydantic==2.11.7\" \\\n",
        "  \"numpy==2.0.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGR60MC5QZPd"
      },
      "outputs": [],
      "source": [
        "# ÏΩîÎû© ÌôòÍ≤ΩÏóêÏÑú Ïã§Ìñâ ÏòàÏãú\n",
        "# ========================\n",
        "# ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] =  \"fill out your API key\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6G1qJ8_yHcw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQDaTo2U4mFO"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# üìå LangGraph RAG (ÏÜåÎπÑÏûêÎ∂ÑÏüÅ) - ÏµúÏ¢Ö Ìï©Î≥∏ (Î¨∏Îã®Ìòï/ÎØ∏Í¥ÑÏãù ÏöîÏïΩ)\n",
        "# ÌùêÎ¶Ñ: ÏßàÎ¨∏ ‚Üí ÎùºÏö∞ÌåÖ(ÏßàÎ¨∏ÌåêÎã®Í∏∞) ‚Üí Ìï¥Í≤∞Í∏∞Ï§Ä + Î≤ïÎ•† ‚Üí (ÏÑúÎπÑÏä§/ÏÉÅÌíà)ÏÇ¨Î°Ä\n",
        "#      ‚Üí Ïª¥Î∞îÏù∏(Î¨∏Îã® Ï¥àÏïà) ‚Üí ÌïúÎ¨∏Îã® ÏöîÏïΩ(ÎØ∏Í¥ÑÏãù) ‚Üí Í≤∞Í≥º\n",
        "# =============================================================================\n",
        "# Colab ÏÑ§Ïπò ÏòàÏãú:\n",
        "# !pip install -q langchain langchain-openai langchain-community langgraph faiss-cpu openai tiktoken\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, re\n",
        "from typing import TypedDict, List, Annotated, Dict, Any\n",
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# ÌôòÍ≤Ω ÏÑ§Ï†ï (API ÌÇ§/Î™®Îç∏)\n",
        "# -----------------------------------------------------------------------------\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    raise RuntimeError(\"OPENAI_API_KEYÎ•º Î®ºÏ†Ä ÏÑ§Ï†ïÌïòÏÑ∏Ïöî. Ïòà) os.environ['OPENAI_API_KEY']='sk-...'\")\n",
        "\n",
        "LLM = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "EMB = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Î≤°ÌÑ∞ DB Í≤ΩÎ°ú\n",
        "# -----------------------------------------------------------------------------\n",
        "DB_GUIDELINE = \"fill out DB path\"\n",
        "DB_LAW       = \"fill out DB path\"\n",
        "DB_CASE_SVC  = \"fill out DB path\"\n",
        "DB_CASE_PRD  = \"fill out DB path\"\n",
        "\n",
        "VG     = FAISS.load_local(DB_GUIDELINE, EMB, allow_dangerous_deserialization=True)\n",
        "VL     = FAISS.load_local(DB_LAW,       EMB, allow_dangerous_deserialization=True)\n",
        "VCSVC  = FAISS.load_local(DB_CASE_SVC,  EMB, allow_dangerous_deserialization=True)\n",
        "VCPRD  = FAISS.load_local(DB_CASE_PRD,  EMB, allow_dangerous_deserialization=True)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Ïù∏Ïä§Ìä∏Îü≠ÏÖò + ÌîÑÎ°¨ÌîÑÌä∏\n",
        "# -----------------------------------------------------------------------------\n",
        "BASE_INSTRUCTION = (\n",
        "    \"ÎãπÏã†ÏùÄ ÏÜåÎπÑÏûêÎ∂ÑÏüÅ Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. Ï£ºÏñ¥ÏßÑ ÏßàÎ¨∏Ïùò ÏöîÏßÄÎ•º ÌååÏïÖÌïòÍ≥† Ï†ÅÏ†àÌïú ÎåÄÎãµÏùÑ Ìï¥Ï£ºÏÑ∏Ïöî.\\n\"\n",
        "    \"Ï£ºÏñ¥ÏßÑ ÏßàÎ¨∏Ïóê ÎåÄÌï¥ÏÑú ÏÇ¨Ï†ÑÏóê Î∞òÎìúÏãú Ï£ºÏñ¥ÏßÑ Î¨∏ÏÑúÎßåÏùÑ ÌÜ†ÎåÄÎ°ú Í∏∞Ï§Ä ÏßÄÏπ®ÏùÑ ÌååÏïÖÌïú ÌõÑ, \"\n",
        "    \"Î≤ïÎ•†Ï†Å Ï°∞Ïñ∏Ïù¥ ÌïÑÏöîÌïú ÎåÄÎãµÏùÄ Ï†úÏãúÎêú 'ÎØºÎ≤ï', 'ÏÜåÎπÑÏûêÎ≥¥Ìò∏Î≤ï', 'Ï†ÑÏûêÏÉÅÍ±∞ÎûòÎ≤ï'Ïùò Î¨∏ÏÑúÎ•º Ï∞∏Ï°∞ÌïòÏó¨ ÎãµÎ≥ÄÏùÑ Íµ¨ÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.\\n\"\n",
        "    \"ÎßåÏïΩ ÏÇ¨Ïö©ÏûêÍ∞Ä ÏÇ¨Î°ÄÎ•º ÏöîÏ≤≠Ìï† Ïãú, Ï†úÏãúÎêú 'Î∂ÑÏüÅÏ°∞Ï†ïÏÇ¨Î°Ä' Î¨∏ÏÑúÎ•º Ï∞∏Ï°∞ÌïòÏó¨ ÏÇ¨Î°ÄÏùò Íµ¨Ï≤¥Ï†Å ÎÇ¥Ïö©ÏùÑ Ìï®Íªò ÎßêÌï¥Ï£ºÏÑ∏Ïöî.\\n\"\n",
        "    \"Ï†ÅÏ†àÌïú ÎãµÎ≥ÄÏù¥ Î∂àÍ∞ÄÎä•Ìï† Ïãú 'I don't know'Î°ú ÎåÄÎãµÌï¥ Ï£ºÏÑ∏Ïöî.\\n\\n\"\n",
        "    \"Ï∂îÏÉÅÏ†Å Îã®Ïñ¥Ïùò Ï≤òÎ¶¨Î∞©Î≤ï\\n\"\n",
        "    \" - ÎπÑÍ≥µÍ∞ú Ïû•ÏÜå: ÌôîÏû•Ïã§, Í∞úÏù∏ Í∞ùÏã§ Îì±\\n\"\n",
        "    \" - ÎØºÍ∞êÏ†ïÎ≥¥: Í∞úÏù∏ Ï£ºÎØºÎì±Î°ùÎ≤àÌò∏, Ï£ºÏÜå, Ïó∞ÎùΩÏ≤ò Îì±\\n\"\n",
        "    \"ÏúÑÏôÄ Í∞ôÏù¥ ÏßàÎ¨∏Ïùò Ï∂îÏÉÅÏ†ÅÏù∏ Îã®Ïñ¥Îäî ÎÑàÍ∞Ä Î≥¥Ïú†Ìïú ÏßÄÏãùÏ≤¥Í≥Ñ ÎÇ¥ÏóêÏÑú Íµ¨Ï≤¥ÌôîÌïòÏó¨ Í≥†Î†§Ìïú Îã§ÏùåÏóê ÎãµÎ≥ÄÏùÑ Î∂ÄÌÉÅÌï¥.\"\n",
        ")\n",
        "\n",
        "GUIDELINE_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=(\n",
        "        f\"{BASE_INSTRUCTION}\\n\"\n",
        "        \"Îã§ÏùåÏùÄ 'ÏÜåÎπÑÏûêÎ∂ÑÏüÅ Ìï¥Í≤∞Í∏∞Ï§Ä(Í∞úÏ†ïÏïà)' Î¨∏ÏÑú Î∞úÏ∑åÏûÖÎãàÎã§:\\n\\n\"\n",
        "        \"{context}\\n\\n\"\n",
        "        \"ÏßàÎ¨∏: {question}\\n\\n\"\n",
        "        \"‚Üí (1) Ï≤òÎ¶¨ÏõêÏπô/Ï±ÖÏûÑÏ£ºÏ≤¥, (2) ÍµêÌôò¬∑ÌôòÎ∂à¬∑ÏàòÎ¶¨ Îì± Î≥¥ÏÉÅÍ∏∞Ï§Ä, \"\n",
        "        \"(3) Ï¶ùÎπô/Ï†àÏ∞®, (4) ÏòàÏô∏¬∑Ïú†ÏùòÏÇ¨Ìï≠ÏùÑ Ìï≠Î™©Î≥ÑÎ°ú Ï†úÏãúÌïòÏÑ∏Ïöî.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "LAW_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=(\n",
        "        f\"{BASE_INSTRUCTION}\\n\"\n",
        "        \"Î≤ïÎ•† Î≤îÏúÑÎäî ÎØºÎ≤ï, ÏÜåÎπÑÏûêÍ∏∞Î≥∏Î≤ï, Ï†ÑÏûêÏÉÅÍ±∞ÎûòÎ≤ïÏúºÎ°ú ÌïúÏ†ïÌï©ÎãàÎã§.\\n\"\n",
        "        \"Îã§ÏùåÏùÄ Î≤ïÎ•†/ÏãúÌñâÎ†π Î¨∏ÏÑú Î∞úÏ∑åÏûÖÎãàÎã§:\\n\\n\"\n",
        "        \"{context}\\n\\n\"\n",
        "        \"ÏßàÎ¨∏: {question}\\n\\n\"\n",
        "        \"Ï∂úÎ†• ÌòïÏãù(Î∞òÎìúÏãú Ï§ÄÏàò):\\n\"\n",
        "        \"[Ï†ÅÏö© Î≤ïÏ°∞Î¨∏]\\n\"\n",
        "        \"- Î≤ïÎ•†Î™Ö Ï†únÏ°∞ Ï†úmÌï≠: ÏöîÏßÄ\\n\"\n",
        "        \"- (ÌïÑÏöî Ïãú) Î≤ïÎ•†Î™Ö Ï†úpÏ°∞: ÏöîÏßÄ\\n\"\n",
        "        \"[Î∂ÑÏÑù]\\n\"\n",
        "        \"- ÏöîÍ±¥/ÏùòÎ¨¥/Í∏àÏßÄ\\n\"\n",
        "        \"- ÏúÑÎ∞ò Ïãú Ï°∞Ïπò(Í≥ºÌÉúÎ£å/ÏÜêÌï¥Î∞∞ÏÉÅ Îì±)\\n\\n\"\n",
        "        \"Ï£ºÏùò: ÏµúÏÜå 1Í∞ú Ïù¥ÏÉÅÏùò Ï†ïÌôïÌïú Ï°∞Î¨∏ ÌëúÍ∏∞Î•º Ìè¨Ìï®ÌïòÍ≥†, Î¨∏ÏÑúÏóê ÏóÜÎäî Ï°∞Î¨∏ÏùÑ Ï∂îÏ†ïÌïòÏßÄ ÎßàÏÑ∏Ïöî.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "CASE_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\", \"category\"],\n",
        "    template=(\n",
        "        f\"{BASE_INSTRUCTION}\\n\"\n",
        "        \"Îã§ÏùåÏùÄ '{category}' Ïπ¥ÌÖåÍ≥†Î¶¨Ïùò ÏÜåÎπÑÏûêÎ∂ÑÏüÅ ÏÇ¨Î°Ä Î∞úÏ∑åÏûÖÎãàÎã§:\\n\\n\"\n",
        "        \"{context}\\n\\n\"\n",
        "        \"ÏßàÎ¨∏: {question}\\n\\n\"\n",
        "        \"‚Üí (1) ÏÇ¨Ïã§Í¥ÄÍ≥Ñ, (2) ÏüÅÏ†ê, (3) ÌåêÎã®/Ï°∞Ï†ïÍ≤∞Í≥º, (4) Ï†ÅÏö© Ìè¨Ïù∏Ìä∏Î•º Í∞ÑÍ≤∞Ìûà ÏöîÏïΩÌïòÏÑ∏Ïöî.\\n\"\n",
        "        \"‚Äª Î∞òÎìúÏãú '**ÌïúÍµ≠ÏÜåÎπÑÏûêÏõê Ï†úÍ≥µ ÏÇ¨Î°ÄÏóê Îî∞Î•¥Î©¥**'ÏúºÎ°ú ÏãúÏûëÌïòÏÑ∏Ïöî.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3) ÏÉÅÌÉú Ï†ïÏùò\n",
        "# -----------------------------------------------------------------------------\n",
        "class QAState(TypedDict):\n",
        "    query: Annotated[str, \"query\"]\n",
        "    need_guideline: bool\n",
        "    need_law: bool\n",
        "    need_case_service: bool\n",
        "    need_case_product: bool\n",
        "    result_guideline: str\n",
        "    result_law: str\n",
        "    result_case_service: str\n",
        "    result_case_product: str\n",
        "    law_citations: List[str]\n",
        "    combined: str\n",
        "    summary: str\n",
        "    source_documents_guideline: List\n",
        "    source_documents_law: List\n",
        "    source_documents_case_service: List\n",
        "    source_documents_case_product: List\n",
        "    source_documents: List\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Î¶¨Ìä∏Î¶¨Î≤Ñ & Ï≤¥Ïù∏\n",
        "# -----------------------------------------------------------------------------\n",
        "ret_gdl = VG.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2, \"fetch_k\": 7})\n",
        "ret_law = VL.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2, \"fetch_k\": 7})\n",
        "ret_csv = VCSVC.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2, \"fetch_k\": 7})\n",
        "ret_cpr = VCPRD.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2, \"fetch_k\": 7})\n",
        "\n",
        "chain_gdl = RetrievalQA.from_chain_type(\n",
        "    llm=LLM, retriever=ret_gdl, return_source_documents=True,\n",
        "    chain_type=\"stuff\", chain_type_kwargs={\"prompt\": GUIDELINE_PROMPT},\n",
        ")\n",
        "chain_law = RetrievalQA.from_chain_type(\n",
        "    llm=LLM, retriever=ret_law, return_source_documents=True,\n",
        "    chain_type=\"stuff\", chain_type_kwargs={\"prompt\": LAW_PROMPT},\n",
        ")\n",
        "chain_csv = RetrievalQA.from_chain_type(\n",
        "    llm=LLM, retriever=ret_csv, return_source_documents=True,\n",
        "    chain_type=\"stuff\", chain_type_kwargs={\"prompt\": CASE_PROMPT.partial(category=\"ÏÑúÎπÑÏä§\")},\n",
        ")\n",
        "chain_cpr = RetrievalQA.from_chain_type(\n",
        "    llm=LLM, retriever=ret_cpr, return_source_documents=True,\n",
        "    chain_type=\"stuff\", chain_type_kwargs={\"prompt\": CASE_PROMPT.partial(category=\"ÏÉÅÌíà\")},\n",
        ")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Ï°∞Î¨∏ ÏûêÎèô Ï∂îÏ∂ú\n",
        "# -----------------------------------------------------------------------------\n",
        "COMBINED_ARTICLE_RE = re.compile(\n",
        "    r\"(Ï†ú\\s*\\d+\\s*Ï°∞(?:\\s*Ïùò\\s*\\d+)?)\"\n",
        "    r\"(?:\\s*(?:[,¬∑]|\\s)?\\s*Ï†ú\\s*(\\d+)\\s*Ìï≠)?\",\n",
        "    re.UNICODE\n",
        ")\n",
        "\n",
        "def extract_statute_citations(source_docs: List) -> List[str]:\n",
        "    found: List[str] = []\n",
        "    seen: set = set()\n",
        "    for d in source_docs or []:\n",
        "        text = getattr(d, \"page_content\", \"\") or \"\"\n",
        "        meta = getattr(d, \"metadata\", {}) or {}\n",
        "        law_name = (meta.get(\"law_name\") or \"\").strip()\n",
        "\n",
        "        for m in COMBINED_ARTICLE_RE.finditer(text):\n",
        "            article = (m.group(1) or \"\").strip()\n",
        "            clause  = (m.group(2) or \"\").strip()\n",
        "            clause_str = f\" Ï†ú{clause}Ìï≠\" if clause else \"\"\n",
        "            prefix = f\"{law_name} \" if law_name else \"\"\n",
        "            citation = f\"{prefix}{article}{clause_str}\".replace(\"  \", \" \")\n",
        "            if citation and citation not in seen:\n",
        "                seen.add(citation)\n",
        "                found.append(citation)\n",
        "                if len(found) >= 8:\n",
        "                    return found\n",
        "    return found\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# ÏßàÎ¨∏ ÌåêÎã®Í∏∞(ÎùºÏö∞ÌåÖ)\n",
        "# -----------------------------------------------------------------------------\n",
        "OVERRIDE_BOTH   = [\"[both]\", \"[Ï¢ÖÌï©]\", \"[all]\"]\n",
        "OVERRIDE_LAW    = [\"[Î≤ï]\", \"[law]\", \"[Ï°∞Î¨∏]\", \"[ÎØºÎ≤ï]\", \"[ÏÜåÎπÑÏûêÎ≤ï]\", \"[Ï†ÑÏûêÏÉÅÍ±∞ÎûòÎ≤ï]\", \"[Î≤ïÎ¶¨]\", \"[legal]\"]\n",
        "OVERRIDE_CASE   = [\"[ÏÇ¨Î°Ä]\", \"[case]\", \"[precedent]\"]\n",
        "OVERRIDE_SVC    = [\"[ÏÑúÎπÑÏä§]\", \"[service]\"]\n",
        "OVERRIDE_PRD    = [\"[ÏÉÅÌíà]\", \"[product]\"]\n",
        "OVERRIDE_GDL    = [\"[Í∏∞Ï§Ä]\", \"[guideline]\", \"[Ìï¥Í≤∞Í∏∞Ï§Ä]\"]\n",
        "\n",
        "NEG_CASE_TAGS   = [\"[nocase]\", \"[case_off]\"]\n",
        "NEG_LAW_TAGS    = [\"[nolaw]\", \"[law_off]\"]\n",
        "NEG_GDL_TAGS    = [\"[noguideline]\", \"[gdl_off]\"]\n",
        "NO_CASE_PHRASE  = [\"ÏÇ¨Î°ÄÎäî Îπº\", \"ÏÇ¨Î°Ä ÎπºÍ≥†\", \"ÏÇ¨Î°Ä Ï†úÏô∏\", \"Ïú†ÏÇ¨ÏÇ¨Î°Ä Ï†úÏô∏\"]\n",
        "NO_LAW_PHRASE   = [\"Î≤ïÎ•†ÏùÄ Îπº\", \"Î≤ïÏùÄ ÎπºÍ≥†\", \"Ï°∞Î¨∏ Ï†úÏô∏\", \"Î≤ïÎ¶¨ Ï†úÏô∏\"]\n",
        "NO_GDL_PHRASE   = [\"Ìï¥Í≤∞Í∏∞Ï§ÄÏùÄ Îπº\", \"Í∏∞Ï§Ä Ï†úÏô∏\", \"Í∏∞Ï§ÄÏùÄ ÎπºÍ≥†\"]\n",
        "\n",
        "BOTH_HINTS = [\"Î≤ïÎ•†Í≥º ÏÇ¨Î°Ä Î™®Îëê\", \"Î≤ïÍ≥º ÏÇ¨Î°Ä Î™®Îëê\", \"Îëò Îã§\", \"Ï¢ÖÌï© Í≤ÄÌÜ†\", \"Ìï®Íªò Í≤ÄÌÜ†\", \"ÎèôÏãúÏóê\", \"ÎπÑÍµêÌï¥ÏÑú\", \"Ï¢ÖÌï©Ìï¥ÏÑú\"]\n",
        "GDL_HINTS  = [\"Î≥¥ÏÉÅÍ∏∞Ï§Ä\",\"ÍµêÌôò\",\"ÌôòÎ∂à\",\"ÏàòÎ¶¨\",\"AS\",\"Ï≤≠ÏïΩÏ≤†Ìöå\",\"Î∞òÌíà\",\"Ï∑®ÏÜå\",\"ÏúÑÏïΩÍ∏à\",\"ÏàòÏàòÎ£å\",\"ÏßÄÏó∞Î∞∞ÏÉÅ\",\n",
        "              \"Ï≤òÎ¶¨ÏõêÏπô\",\"Ï±ÖÏûÑÏ£ºÏ≤¥\",\"Ï†àÏ∞®\",\"Ï¶ùÎπô\",\"Í∏∞Í∞Ñ\",\"Í∏∞Ìïú\",\"ÏòàÏô∏\",\"Ïú†ÏùòÏÇ¨Ìï≠\"]\n",
        "LAW_HINTS  = [\"Î≤ïÎ•†\",\"Î≤ïÎ†π\",\"Ï°∞Î¨∏\",\"Í∑ºÍ±∞\",\"ÏãúÌñâÎ†π\",\"ÏãúÌñâÍ∑úÏπô\",\"Í∑úÏ†ï\",\"Î≤åÏπô\",\"Í≥ºÌÉúÎ£å\",\"ÎØºÏÇ¨\",\"ÌòïÏÇ¨\",\n",
        "              \"ÏÜêÌï¥Î∞∞ÏÉÅ\",\"Î∞∞ÏÉÅÏ±ÖÏûÑ\",\"ÏûÖÏ¶ùÏ±ÖÏûÑ\",\"ÏÜåÎ©∏ÏãúÌö®\",\"Ï†úÏ≤ôÍ∏∞Í∞Ñ\",\"Í¥ÄÌï†\",\"ÏïΩÍ¥Ä\",\"Î∂àÍ≥µÏ†ïÏïΩÍ¥Ä\",\n",
        "              \"ÎØºÎ≤ï\",\"ÏÜåÎπÑÏûêÍ∏∞Î≥∏Î≤ï\",\"Ï†ÑÏûêÏÉÅÍ±∞ÎûòÎ≤ï\",\"Ï†ÑÏûêÏÉÅÍ±∞Îûò Îì±ÏóêÏÑúÏùò ÏÜåÎπÑÏûêÎ≥¥Ìò∏\"]\n",
        "LAW_STRICT = [\"Î≤ïÎ¶¨\",\"Î≤ïÎ•†Ï†ÅÏúºÎ°ú\",\"Î≤ïÎ•† ÏúÑÏ£º\",\"Î≤ïÏ†Å Í≤ÄÌÜ†\",\"Î≤ïÏ†Å ÌåêÎã®\",\"Ï°∞Î¨∏ Ï§ëÏã¨\",\"Î≤ïÎ•† Í≤ÄÌÜ†\"]\n",
        "CASE_HINTS = [\"Ïú†ÏÇ¨ÏÇ¨Î°Ä\",\"ÏÇ¨Î°Ä\",\"Î∂ÑÏüÅÏÇ¨Î°Ä\",\"ÌåêÏ†ï\",\"Ï°∞Ï†ï\",\"ÌåêÎ°Ä\",\"Í≤∞Ï†ï\",\"ÏÇ¨Ïã§Í¥ÄÍ≥Ñ Ïú†ÏÇ¨\"]\n",
        "\n",
        "SERVICE_HINTS = [\"Ïó¨Ìñâ\",\"Ìï≠Í≥µ\",\"Ìï≠Í≥µÍ∂å\",\"Ìï≠Í≥µÍ∏∞\",\"ÏßÄÏó∞\",\"Í≤∞Ìï≠\",\"ÏàôÎ∞ï\",\"Ìò∏ÌÖî\",\"ÌÉùÎ∞∞\",\"Î∞∞ÏÜ°ÏßÄÏó∞\",\"Ïö¥ÏÜ°\",\n",
        "                 \"ÏàòÍ∞ï\",\"ÌïôÏõê\",\"Í∞ïÏùò\",\"Íµ¨ÎèÖ\",\"Î©§Î≤ÑÏã≠\",\"ÌÜµÏã†\",\"Ïù∏ÌÑ∞ÎÑ∑\",\"ÏÑ§Ïπò\",\"ÏãúÍ≥µ\",\"ÏãúÏà†\",\"ÎØ∏Ïö©\",\"ÏàòÎ¶¨\",\"Ïö©Ïó≠\",\"ÌîåÎû´Ìèº\"]\n",
        "PRODUCT_HINTS = [\"ÏÉÅÌíà\",\"Ï†úÌíà\",\"Í∞ÄÏ†Ñ\",\"Ï†ÑÏûêÏ†úÌíà\",\"Ìú¥ÎåÄÌè∞\",\"ÎÖ∏Ìä∏Î∂Å\",\"TV\",\"ÎÉâÏû•Í≥†\",\"ÏÑ∏ÌÉÅÍ∏∞\",\"ÏùòÎ•ò\",\"Ïã†Î∞ú\",\n",
        "                 \"ÏãùÌíà\",\"ÌôîÏû•Ìíà\",\"Í∞ÄÍµ¨\",\"Ïú†ÏïÑÏö©Ìíà\",\"ÏôÑÍµ¨\",\"ÌïòÏûê\",\"Î∂àÎüâ\",\"Ï¥àÍ∏∞Î∂àÎüâ\",\"Î∂ÄÌíà\",\"ÍµêÌôòÏöîÏ≤≠\"]\n",
        "\n",
        "def _has_any(tokens: List[str], text: str) -> bool:\n",
        "    return any(tok in text for tok in tokens)\n",
        "\n",
        "def classify_needs(q: str) -> Dict[str, bool]:\n",
        "    q = (q or \"\").strip()\n",
        "    need_guideline = True\n",
        "    need_law = False\n",
        "    need_case_service = False\n",
        "    need_case_product = False\n",
        "\n",
        "    # Î∞∞Ï†ú\n",
        "    if _has_any(NEG_GDL_TAGS, q) or _has_any(NO_GDL_PHRASE, q): need_guideline = False\n",
        "    if _has_any(NEG_LAW_TAGS, q) or _has_any(NO_LAW_PHRASE, q): need_law = False\n",
        "    if _has_any(NEG_CASE_TAGS, q) or _has_any(NO_CASE_PHRASE, q):\n",
        "        need_case_service = need_case_product = False\n",
        "\n",
        "    # Î™ÖÏãú\n",
        "    if _has_any(OVERRIDE_BOTH, q) or _has_any(BOTH_HINTS, q):\n",
        "        need_law = True\n",
        "        if _has_any(OVERRIDE_SVC + SERVICE_HINTS, q): need_case_service = True\n",
        "        if _has_any(OVERRIDE_PRD + PRODUCT_HINTS, q): need_case_product = True\n",
        "        if not (need_case_service or need_case_product): need_case_service = True\n",
        "        return {\"need_guideline\": need_guideline, \"need_law\": need_law,\n",
        "                \"need_case_service\": need_case_service, \"need_case_product\": need_case_product}\n",
        "\n",
        "    if _has_any(OVERRIDE_GDL, q) and not (_has_any(OVERRIDE_LAW, q) or _has_any(OVERRIDE_CASE, q)):\n",
        "        need_law = need_case_service = need_case_product = False\n",
        "        return {\"need_guideline\": need_guideline, \"need_law\": need_law,\n",
        "                \"need_case_service\": need_case_service, \"need_case_product\": need_case_product}\n",
        "\n",
        "    if _has_any(OVERRIDE_LAW, q):\n",
        "        need_law = True\n",
        "        if _has_any(OVERRIDE_SVC, q): need_case_service = True\n",
        "        if _has_any(OVERRIDE_PRD, q): need_case_product = True\n",
        "        return {\"need_guideline\": need_guideline, \"need_law\": need_law,\n",
        "                \"need_case_service\": need_case_service, \"need_case_product\": need_case_product}\n",
        "\n",
        "    if _has_any(OVERRIDE_CASE, q):\n",
        "        if _has_any(OVERRIDE_SVC + SERVICE_HINTS, q): need_case_service = True\n",
        "        if _has_any(OVERRIDE_PRD + PRODUCT_HINTS, q): need_case_product = True\n",
        "        if not (need_case_service or need_case_product): need_case_service = True\n",
        "        need_law = False\n",
        "        return {\"need_guideline\": need_guideline, \"need_law\": need_law,\n",
        "                \"need_case_service\": need_case_service, \"need_case_product\": need_case_product}\n",
        "\n",
        "    # ÏûêÏó∞Ïñ¥ ÏùòÎèÑ\n",
        "    if _has_any(LAW_STRICT, q):\n",
        "        need_law = True\n",
        "        if _has_any(CASE_HINTS, q):\n",
        "            if _has_any(SERVICE_HINTS, q): need_case_service = True\n",
        "            elif _has_any(PRODUCT_HINTS, q): need_case_product = True\n",
        "        else:\n",
        "            need_case_service = need_case_product = False\n",
        "        return {\"need_guideline\": need_guideline, \"need_law\": need_law,\n",
        "                \"need_case_service\": need_case_service, \"need_case_product\": need_case_product}\n",
        "\n",
        "    if _has_any(GDL_HINTS, q):\n",
        "        if _has_any(LAW_HINTS, q): need_law = True\n",
        "\n",
        "    if _has_any(CASE_HINTS, q):\n",
        "        if _has_any(SERVICE_HINTS, q): need_case_service = True\n",
        "        elif _has_any(PRODUCT_HINTS, q): need_case_product = True\n",
        "        else: need_case_service = True\n",
        "        if _has_any(LAW_HINTS, q): need_law = True\n",
        "\n",
        "    # Í∏∞Î≥∏ Ìè¥Î∞±\n",
        "    if not (need_law or need_case_service or need_case_product):\n",
        "        need_law = True\n",
        "        if _has_any(SERVICE_HINTS, q): need_case_service = True\n",
        "        elif _has_any(PRODUCT_HINTS, q): need_case_product = True\n",
        "\n",
        "    # ÏµúÏ¢Ö Î∞∞Ï†ú Ïû¨Ï†ÅÏö©\n",
        "    if _has_any(NEG_LAW_TAGS, q) or _has_any(NO_LAW_PHRASE, q): need_law = False\n",
        "    if _has_any(NEG_CASE_TAGS, q) or _has_any(NO_CASE_PHRASE, q):\n",
        "        need_case_service = need_case_product = False\n",
        "    if _has_any(NEG_GDL_TAGS, q) or _has_any(NO_GDL_PHRASE, q): need_guideline = False\n",
        "\n",
        "    return {\"need_guideline\": need_guideline, \"need_law\": need_law,\n",
        "            \"need_case_service\": need_case_service, \"need_case_product\": need_case_product}\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# ÎÖ∏ÎìúÎì§\n",
        "# -----------------------------------------------------------------------------\n",
        "def router_node(state: QAState) -> QAState:\n",
        "    return {**state, **classify_needs(state[\"query\"])}\n",
        "\n",
        "def guideline_node(state: QAState) -> QAState:\n",
        "    if not state.get(\"need_guideline\", True):\n",
        "        return {**state, \"result_guideline\": \"\", \"source_documents_guideline\": []}\n",
        "    r = chain_gdl.invoke({\"query\": state[\"query\"]})\n",
        "    return {**state, \"result_guideline\": r[\"result\"], \"source_documents_guideline\": r.get(\"source_documents\", [])}\n",
        "\n",
        "def law_node(state: QAState) -> QAState:\n",
        "    if not state.get(\"need_law\", False):\n",
        "        return {**state, \"result_law\": \"\", \"source_documents_law\": [], \"law_citations\": []}\n",
        "    r = chain_law.invoke({\"query\": state[\"query\"]})\n",
        "    cites = extract_statute_citations(r.get(\"source_documents\", []))\n",
        "    return {\n",
        "        **state,\n",
        "        \"result_law\": r[\"result\"],\n",
        "        \"source_documents_law\": r.get(\"source_documents\", []),\n",
        "        \"law_citations\": cites,\n",
        "    }\n",
        "\n",
        "def case_service_node(state: QAState) -> QAState:\n",
        "    if not state.get(\"need_case_service\", False):\n",
        "        return {**state, \"result_case_service\": \"\", \"source_documents_case_service\": []}\n",
        "    r = chain_csv.invoke({\"query\": state[\"query\"]})\n",
        "    return {**state, \"result_case_service\": r[\"result\"], \"source_documents_case_service\": r.get(\"source_documents\", [])}\n",
        "\n",
        "def case_product_node(state: QAState) -> QAState:\n",
        "    if not state.get(\"need_case_product\", False):\n",
        "        return {**state, \"result_case_product\": \"\", \"source_documents_case_product\": []}\n",
        "    r = chain_cpr.invoke({\"query\": state[\"query\"]})\n",
        "    return {**state, \"result_case_product\": r[\"result\"], \"source_documents_case_product\": r.get(\"source_documents\", [])}\n",
        "\n",
        "# ---------- Ïª¥Î∞îÏù∏ ----------\n",
        "def combine_node(state: QAState) -> QAState:\n",
        "    parts = []\n",
        "    if state.get(\"result_guideline\"):     parts.append(state[\"result_guideline\"])\n",
        "    if state.get(\"result_law\"):           parts.append(state[\"result_law\"])\n",
        "    if state.get(\"result_case_service\"):  parts.append(state[\"result_case_service\"])\n",
        "    if state.get(\"result_case_product\"):  parts.append(state[\"result_case_product\"])\n",
        "\n",
        "    cites = state.get(\"law_citations\") or []\n",
        "    cites_line = \", \".join(cites)\n",
        "\n",
        "    if not parts:\n",
        "        combined_text = \"I don't know\"\n",
        "    else:\n",
        "        material = \"\\n\\n\".join(parts)\n",
        "        prompt = (\n",
        "            \"ÏïÑÎûò ÏûêÎ£åÎßåÏùÑ Í∑ºÍ±∞Î°ú, ÏÜåÎπÑÏûêÎ∂ÑÏüÅ Ìï¥Í≤∞Í∏∞Ï§ÄÍ≥º Í¥ÄÎ†® Î≤ïÎ•†, Í∑∏Î¶¨Í≥† (ÏûàÎã§Î©¥) Ïú†ÏÇ¨ÏÇ¨Î°ÄÎ•º \"\n",
        "            \"ÏûêÏó∞Ïä§Îü¨Ïö¥ ÌïúÍµ≠Ïñ¥ Î¨∏Îã®ÏúºÎ°ú ÌÜµÌï© ÏöîÏïΩÌïòÎùº. ÏÑπÏÖò ÎùºÎ≤®(Ïòà: 'Î≤ïÎ•†:' Îì±)ÏùÄ Ïì∞ÏßÄ ÎßêÍ≥†, \"\n",
        "            \"ÏµúÎåÄ Îëê Î¨∏Îã®ÏúºÎ°ú Í∞ÑÍ≤∞ÌïòÍ≤å Ï†ïÎ¶¨ÌïòÎùº. ÏÇ¨Ïã§/ÏàòÏπò/Ï°∞Î¨∏ÏùÄ ÏûêÎ£åÏóê ÏûàÎäî Í≤ÉÎßå Ïì∞Í≥† Ï∂îÏ†ï Í∏àÏßÄ.\\n\"\n",
        "            f\"{('Ï∞∏Í≥† Ï°∞Î¨∏: ' + cites_line) if cites_line else ''}\\n\\n\"\n",
        "            \"ÏûêÎ£å:\\n\" + material\n",
        "        )\n",
        "        combined_text = LLM.invoke(prompt).content\n",
        "\n",
        "    all_sources = []\n",
        "    all_sources += state.get(\"source_documents_guideline\", [])\n",
        "    all_sources += state.get(\"source_documents_law\", [])\n",
        "    all_sources += state.get(\"source_documents_case_service\", [])\n",
        "    all_sources += state.get(\"source_documents_case_product\", [])\n",
        "    return {**state, \"combined\": combined_text, \"source_documents\": all_sources}\n",
        "\n",
        "# ---------- Ìïú Î¨∏Îã® ÏöîÏïΩ ----------\n",
        "def summarize_text_prose(text: str, *,\n",
        "                         have_guideline: bool,\n",
        "                         have_law: bool,\n",
        "                         have_cases: bool,\n",
        "                         citations: List[str]) -> str:\n",
        "    cite_str = \", \".join(citations) if citations else \"\"\n",
        "    prompt = (\n",
        "        \"ÏïÑÎûò ÌÜµÌï© Ï¥àÏïàÎßåÏùÑ Í∑ºÍ±∞Î°ú, ÏµúÏ¥àÏóê Ï†úÏãúÎêú ÏßàÎ¨∏Ïùò ÏöîÏßÄÏóê ÎßûÍ≤å ÌïúÍµ≠Ïñ¥Î°ú Ìïú Î¨∏Îã®Ïùò ÏûêÏó∞Ïä§Îü¨Ïö¥ ÏùòÍ≤¨ÏÑúÎ•º ÏûëÏÑ±ÌïòÎùº.\\n\"\n",
        "        \"ÏöîÍµ¨ÏÇ¨Ìï≠:\\n\"\n",
        "        \"1) ÏÑπÏÖò ÎùºÎ≤®/Î∂àÎ¶ø/Î≤àÌò∏ Í∏àÏßÄ, ÏôÑÏ†ÑÌïú Î¨∏Ïû•Îì§Î°úÎßå Íµ¨ÏÑ±\\n\"\n",
        "        \"2) Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî Ï†ïÎ≥¥(Î≤ïÏ°∞Î¨∏ Î≤àÌò∏, ÏÇ¨Î°Ä ÏöîÏßÄ, ÏàòÏπò)Îäî Ï†àÎåÄ Ï∂îÏ†ï/Ï∞ΩÏûëÌïòÏßÄ ÎßêÍ≥† Ïñ∏Í∏âÌïòÏßÄ Îßê Í≤É\\n\"\n",
        "        \"3) Î≤ïÏ°∞Î¨∏Ïù¥ ÏûàÏúºÎ©¥ Î¨∏Ïû• Ï§ëÏóê ÏûêÏó∞Ïä§ÎüΩÍ≤å Í¥ÑÌò∏ Îì±ÏúºÎ°ú ÎÖπÏó¨ Ïì∞Îêò, ÏóÜÏúºÎ©¥ Î≤ïÏ°∞Î¨∏ Ïñ∏Í∏â ÏûêÏ≤¥Î•º ÏÉùÎûµ\\n\"\n",
        "        \"4) ÏÇ¨Î°ÄÍ∞Ä ÏûàÏúºÎ©¥ 'Ïú†ÏÇ¨Ìïú Î∂ÑÏüÅÏ°∞Ï†ï ÏÇ¨Î°ÄÏóê ÎπÑÏ∂îÏñ¥' ÏàòÏ§ÄÏúºÎ°ú ÏûêÏó∞Ïä§ÎüΩÍ≤å ÎÖπÏó¨ Ïì∞Îêò, ÏóÜÏúºÎ©¥ ÏÇ¨Î°Ä Ïñ∏Í∏â ÏÉùÎûµ\\n\"\n",
        "        \"5) Ï¥ù 5~10Î¨∏Ïû•, Í≥ºÎèÑÌïú Î∞òÎ≥µ/Ïû•Ìô©Ìï® Í∏àÏßÄ\\n\"\n",
        "        f\"{('Ï∞∏Í≥† Í∞ÄÎä•Ìïú Ï°∞Î¨∏: ' + cite_str) if cite_str else ''}\\n\\n\"\n",
        "        \"ÌÜµÌï© Ï¥àÏïà:\\n\" + text\n",
        "    )\n",
        "    return LLM.invoke(prompt).content\n",
        "\n",
        "def summarization_node(state: QAState) -> QAState:\n",
        "    combined = state.get(\"combined\") or \"\"\n",
        "    if not combined.strip():\n",
        "        return {**state, \"summary\": \"I don't know\"}\n",
        "\n",
        "    have_guideline = bool(state.get(\"result_guideline\"))\n",
        "    have_law       = bool(state.get(\"result_law\"))\n",
        "    have_cases     = bool(state.get(\"result_case_service\") or state.get(\"result_case_product\"))\n",
        "    citations      = state.get(\"law_citations\") or []\n",
        "\n",
        "    summary = summarize_text_prose(\n",
        "        combined,\n",
        "        have_guideline=have_guideline,\n",
        "        have_law=have_law,\n",
        "        have_cases=have_cases,\n",
        "        citations=citations\n",
        "    )\n",
        "\n",
        "    return {**state, \"summary\": summary}\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Í∑∏ÎûòÌîÑ Íµ¨ÏÑ± (ÎÖ∏ÎìúÎ™Ö 'summary' ‚Üí 'summarize_node'Î°ú Î≥ÄÍ≤Ω)\n",
        "# -----------------------------------------------------------------------------\n",
        "def build_graph() -> Any:\n",
        "    g = StateGraph(QAState)\n",
        "    g.add_node(\"router\", router_node)\n",
        "    g.add_node(\"guideline\", guideline_node)\n",
        "    g.add_node(\"law\",       law_node)\n",
        "    g.add_node(\"case_service\", case_service_node)\n",
        "    g.add_node(\"case_product\", case_product_node)\n",
        "    g.add_node(\"combine\", combine_node)\n",
        "    g.add_node(\"summarize_node\", summarization_node)  # ‚Üê Î≥ÄÍ≤Ω\n",
        "\n",
        "    g.set_entry_point(\"router\")\n",
        "    g.add_edge(\"router\", \"guideline\")\n",
        "\n",
        "    # Ìï¥Í≤∞Í∏∞Ï§Ä ‚Üí (Î≤ïÎ•†/ÏÇ¨Î°Ä/Ïª¥Î∞îÏù∏)\n",
        "    def after_guideline(state: QAState) -> str:\n",
        "        if state.get(\"need_law\"): return \"to_law\"\n",
        "        if state.get(\"need_case_service\"): return \"to_case_service\"\n",
        "        if state.get(\"need_case_product\"): return \"to_case_product\"\n",
        "        return \"to_combine\"\n",
        "\n",
        "    g.add_conditional_edges(\"guideline\", after_guideline, {\n",
        "        \"to_law\": \"law\",\n",
        "        \"to_case_service\": \"case_service\",\n",
        "        \"to_case_product\": \"case_product\",\n",
        "        \"to_combine\": \"combine\",\n",
        "    })\n",
        "\n",
        "    # Î≤ïÎ•† ‚Üí (ÏÇ¨Î°Ä/Ïª¥Î∞îÏù∏)\n",
        "    def after_law(state: QAState) -> str:\n",
        "        if state.get(\"need_case_service\"): return \"to_case_service\"\n",
        "        if state.get(\"need_case_product\"): return \"to_case_product\"\n",
        "        return \"to_combine\"\n",
        "\n",
        "    g.add_conditional_edges(\"law\", after_law, {\n",
        "        \"to_case_service\": \"case_service\",\n",
        "        \"to_case_product\": \"case_product\",\n",
        "        \"to_combine\": \"combine\",\n",
        "    })\n",
        "\n",
        "    # ÏÑúÎπÑÏä§ ÏÇ¨Î°Ä ‚Üí (ÏÉÅÌíà ÏÇ¨Î°Ä/Ïª¥Î∞îÏù∏)\n",
        "    g.add_conditional_edges(\n",
        "        \"case_service\",\n",
        "        lambda s: \"to_case_product\" if s.get(\"need_case_product\") else \"to_combine\",\n",
        "        {\"to_case_product\": \"case_product\", \"to_combine\": \"combine\"},\n",
        "    )\n",
        "\n",
        "    # ÏÉÅÌíà ÏÇ¨Î°Ä ‚Üí Ïª¥Î∞îÏù∏\n",
        "    g.add_edge(\"case_product\", \"combine\")\n",
        "    # Ïª¥Î∞îÏù∏ ‚Üí ÏöîÏïΩ ‚Üí END\n",
        "    g.add_edge(\"combine\", \"summarize_node\")  # ‚Üê Î≥ÄÍ≤Ω\n",
        "    g.add_edge(\"summarize_node\", END)        # ‚Üê Î≥ÄÍ≤Ω\n",
        "\n",
        "    return g.compile(checkpointer=InMemorySaver())\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Ïã§Ìñâ Ìó¨Ìçº\n",
        "# -----------------------------------------------------------------------------\n",
        "def run_query(graph, question: str, thread_id: str = \"consumer-rag-final-prose\") -> Dict[str, Any]:\n",
        "    init: QAState = {\n",
        "        \"query\": question,\n",
        "        \"need_guideline\": True,\n",
        "        \"need_law\": False,\n",
        "        \"need_case_service\": False,\n",
        "        \"need_case_product\": False,\n",
        "        \"result_guideline\": \"\",\n",
        "        \"result_law\": \"\",\n",
        "        \"result_case_service\": \"\",\n",
        "        \"result_case_product\": \"\",\n",
        "        \"law_citations\": [],\n",
        "        \"combined\": \"\",\n",
        "        \"summary\": \"\",\n",
        "        \"source_documents_guideline\": [],\n",
        "        \"source_documents_law\": [],\n",
        "        \"source_documents_case_service\": [],\n",
        "        \"source_documents_case_product\": [],\n",
        "        \"source_documents\": [],\n",
        "    }\n",
        "    return graph.invoke(init, config={\"configurable\": {\"thread_id\": thread_id}})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2hX9dn8cX_M",
        "outputId": "7c5e62e1-b04a-49fe-f3cb-b71786999ab0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ÏßàÎ¨∏]\n",
            "Ìï¥Ïô∏ ÏßÅÍµ¨ Ï†ÑÏûêÏ†úÌíàÏù¥ Ï¥àÍ∏∞Î∂àÎüâÏùº Îïå ÍµêÌôò/ÌôòÎ∂àÏùÄ Ïñ¥ÎñªÍ≤å ÎêòÎÇòÏöî? Î∞∞ÏÜ°ÎπÑÎäî ÎàÑÍ∞Ä Î∂ÄÎã¥ÌïòÎÇòÏöî?\n",
            "\n",
            "[ÏµúÏ¢Ö ÏöîÏïΩ(ÎØ∏Í¥ÑÏãù Ìïú Î¨∏Îã®)]\n",
            "Ìï¥Ïô∏ ÏßÅÍµ¨ Ï†ÑÏûêÏ†úÌíàÏù¥ Ï¥àÍ∏∞Î∂àÎüâÏù∏ Í≤ΩÏö∞, ÏÜåÎπÑÏûêÎäî ÌåêÎß§ÏûêÏóêÍ≤å ÍµêÌôò ÎòêÎäî ÌôòÎ∂àÏùÑ ÏöîÏ≤≠Ìï† Ïàò ÏûàÏúºÎ©∞, Ïù¥Îïå Î∞úÏÉùÌïòÎäî Î∞∞ÏÜ°ÎπÑÎäî ÏùºÎ∞òÏ†ÅÏúºÎ°ú ÌåêÎß§ÏûêÍ∞Ä Î∂ÄÎã¥ÌïòÎäî Í≤ÉÏù¥ ÏõêÏπôÏûÖÎãàÎã§. Ï†ÑÏûêÏÉÅÍ±∞Îûò Îì±ÏóêÏÑúÏùò ÏÜåÎπÑÏûêÎ≥¥Ìò∏Ïóê Í¥ÄÌïú Î≤ïÎ•†Ïóê Îî∞Î•¥Î©¥, ÏÜåÎπÑÏûêÎäî Ïû¨ÌôîÏùò ÎÇ¥Ïö©Ïù¥ Í¥ëÍ≥†ÏôÄ Îã§Î•¥Í±∞ÎÇò Í≥ÑÏïΩ ÎÇ¥Ïö©Í≥º Îã§Î•¥Í≤å Ïù¥ÌñâÎêú Í≤ΩÏö∞ Ï≤≠ÏïΩÏ≤†ÌöåÎ•º Ìï† Ïàò ÏûàÏäµÎãàÎã§. ÎòêÌïú, ÏÜåÎπÑÏûêÍ∞Ä Ï≤≠ÏïΩÏ≤†ÌöåÎ•º Ìïú Í≤ΩÏö∞ÏóêÎäî Ïù¥ÎØ∏ Í≥µÍ∏âÎ∞õÏùÄ Ïû¨ÌôîÎ•º Î∞òÌôòÌï¥Ïïº ÌïòÎ©∞, ÌåêÎß§ÏûêÎäî ÏÜåÎπÑÏûêÏóêÍ≤å ÏúÑÏïΩÍ∏àÏù¥ÎÇò ÏÜêÌï¥Î∞∞ÏÉÅÏùÑ Ï≤≠Íµ¨Ìï† Ïàò ÏóÜÏäµÎãàÎã§. ÌíàÏßàÎ≥¥Ï¶ùÍ∏∞Í∞Ñ Ïù¥ÎÇ¥Ïóê ÌïòÏûêÍ∞Ä Î∞úÏÉùÌïú Í≤ΩÏö∞, ÏÜåÎπÑÏûêÎäî Ï†úÌíà ÍµêÌôò ÎòêÎäî Íµ¨ÏûÖÍ∞Ä ÌôòÍ∏âÏùÑ ÏöîÏ≤≠Ìï† Ïàò ÏûàÏúºÎ©∞, ÌíàÏßàÎ≥¥Ï¶ùÍ∏∞Í∞ÑÏù¥ Í≤ΩÍ≥ºÌïú Í≤ΩÏö∞ÏóêÎäî Ïú†ÏÉÅÏàòÎ¶¨ ÌõÑ ÍµêÌôòÏù¥ Í∞ÄÎä•Ìï©ÎãàÎã§. Îî∞ÎùºÏÑú ÏÜåÎπÑÏûêÎäî Ï¥àÍ∏∞Î∂àÎüâÏùÑ Ï¶ùÎ™ÖÌï† Ïàò ÏûàÎäî ÏûêÎ£åÎ•º Ï§ÄÎπÑÌïòÏó¨ ÌåêÎß§ÏûêÏóêÍ≤å ÏöîÏ≤≠Ìï¥Ïïº ÌïòÎ©∞, ÌåêÎß§ÏûêÍ∞Ä ÏöîÍµ¨ÌïòÎäî Ï†àÏ∞®Ïóê Îî∞Îùº ÏßÑÌñâÌï¥Ïïº Ìï©ÎãàÎã§. Ïù¥Îü¨Ìïú Ï†àÏ∞®Î•º ÌÜµÌï¥ ÏÜåÎπÑÏûêÎäî ÏûêÏã†Ïùò Í∂åÎ¶¨Î•º Î≥¥Ìò∏Î∞õÏùÑ Ïàò ÏûàÏäµÎãàÎã§.\n",
            "\n",
            "[Ï∞∏Í≥† Ï°∞Î¨∏ ÏûêÎèôÏ∂îÏ∂ú]\n",
            "['Ï†ú17Ï°∞ Ï†ú1Ìï≠', 'Ï†ú17Ï°∞ Ï†ú3Ìï≠', 'Ï†ú19Ï°∞', 'Ï†ú18Ï°∞']\n"
          ]
        }
      ],
      "source": [
        "# Îã®Ïùº ÏßàÎ¨∏ Ïã§Ìñâ ÏΩîÎìú (build_graph(), run_query()Í∞Ä Ïù¥ÎØ∏ Ï†ïÏùòÎêòÏñ¥ ÏûàÏñ¥Ïïº Ìï®)\n",
        "\n",
        "# 1) Í∑∏ÎûòÌîÑ ÎπåÎìú\n",
        "graph = build_graph()\n",
        "\n",
        "# 2) ÏßàÎ¨∏ ÏÑ§Ï†ï\n",
        "question = \"Ìï¥Ïô∏ ÏßÅÍµ¨ Ï†ÑÏûêÏ†úÌíàÏù¥ Ï¥àÍ∏∞Î∂àÎüâÏùº Îïå ÍµêÌôò/ÌôòÎ∂àÏùÄ Ïñ¥ÎñªÍ≤å ÎêòÎÇòÏöî? Î∞∞ÏÜ°ÎπÑÎäî ÎàÑÍ∞Ä Î∂ÄÎã¥ÌïòÎÇòÏöî?\"\n",
        "\n",
        "# 3) Ïã§Ìñâ\n",
        "state = run_query(graph, question=question, thread_id=\"single-run-1\")\n",
        "\n",
        "# 4) Í≤∞Í≥º Ï∂úÎ†•\n",
        "print(\"[ÏßàÎ¨∏]\")\n",
        "print(state[\"query\"])\n",
        "\n",
        "print(\"\\n[ÏµúÏ¢Ö ÏöîÏïΩ(ÎØ∏Í¥ÑÏãù Ìïú Î¨∏Îã®)]\")\n",
        "print(state.get(\"summary\", \"\"))\n",
        "\n",
        "print(\"\\n[Ï∞∏Í≥† Ï°∞Î¨∏ ÏûêÎèôÏ∂îÏ∂ú]\")\n",
        "print(state.get(\"law_citations\", []))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
